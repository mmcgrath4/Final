{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Sentiment Analysis Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Mikey McGrath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mikeymcgrath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mikeymcgrath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mikeymcgrath/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is only from September of the previous season. I can access the other months of other seasons as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the html file and seeing what it looks like\n",
    "f = open(\"data.html\", \"rb\")\n",
    "doc = BeautifulSoup(f, \"html.parser\")\n",
    "doc.prettify()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of NFL teams\n",
    "teams = {\"Bills\" : [],\"Dolphins\": [],\"Patriots\": [],\"Jets\": [],\n",
    "\t\t\t\"Ravens\": [],\"Bengals\": [],\"Browns\": [],\"Steelers\": [],\n",
    "\t\t\t\"Texans\": [],\"Colts\": [],\"Jaguars\": [],\"Titans\": [],\n",
    "\t\t\t\"Broncos\": [],\"Chiefs\": [],\"Raiders\": [],\"Chargers\": [],\n",
    "\t\t\t\"Cowboys\": [],\"Giants\": [],\"Eagles\": [],\"Commanders\": [],\n",
    "\t\t\t\"Bears\": [],\"Lions\": [],\"Packers\": [],\"Vikings\": [],\n",
    "\t\t\t\"Falcons\": [],\"Panthers\": [],\"Saints\": [],\"Buccaneers\": [],\n",
    "\t\t\t\"Cardinals\": [],\"Rams\": [],\"49ers\": [],\"Seahawks\": []}\n",
    "team_scores = {\"Bills\" : [],\"Dolphins\": [],\"Patriots\": [],\"Jets\": [],\n",
    "\t\t\t\"Ravens\": [],\"Bengals\": [],\"Browns\": [],\"Steelers\": [],\n",
    "\t\t\t\"Texans\": [],\"Colts\": [],\"Jaguars\": [],\"Titans\": [],\n",
    "\t\t\t\"Broncos\": [],\"Chiefs\": [],\"Raiders\": [],\"Chargers\": [],\n",
    "\t\t\t\"Cowboys\": [],\"Giants\": [],\"Eagles\": [],\"Commanders\": [],\n",
    "\t\t\t\"Bears\": [],\"Lions\": [],\"Packers\": [],\"Vikings\": [],\n",
    "\t\t\t\"Falcons\": [],\"Panthers\": [],\"Saints\": [],\"Buccaneers\": [],\n",
    "\t\t\t\"Cardinals\": [],\"Rams\": [],\"49ers\": [],\"Seahawks\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "for link in doc.find_all(\"a\"):\n",
    "    text = link.get(\"title\")\n",
    "    if text is not None:\n",
    "        # Getting rid of headlines that don't mention a team\n",
    "        for team in teams:\n",
    "            if team in text and text not in headlines:\n",
    "                headlines.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe find a way to include the cities in case the mascot is not a part of the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each team their headlines\n",
    "for team in teams:\n",
    "    for headline in headlines:\n",
    "        if team in headline:\n",
    "            teams[team].append(headline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the sentiment score of each headline\n",
    "def get_polarity_scores(headline):\n",
    "    scores = sia.polarity_scores(headline)\n",
    "    return scores['neg'], scores['pos'], scores['neu'], scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK ON FLUENCY OF THIS CHUNK\n",
    "def get_avg_polarity_scores(team):\n",
    "    neg_sum = 0\n",
    "    pos_sum = 0\n",
    "    neu_sum = 0\n",
    "    compound_sum = 0\n",
    "    for headline in teams[team]:\n",
    "        scores = get_polarity_scores(headline)\n",
    "        neg_sum += scores[0]\n",
    "        pos_sum += scores[1]\n",
    "        neu_sum += scores[2]\n",
    "        compound_sum += scores[3]\n",
    "    num_headlines = len(teams[team])\n",
    "    neg = neg_sum / num_headlines\n",
    "    pos = pos_sum / num_headlines\n",
    "    neu = neu_sum / num_headlines\n",
    "    compound = compound_sum / num_headlines\n",
    "    return (neg, pos, neu, compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in teams:\n",
    "    team_scores[team].append(get_avg_polarity_scores(team))\n",
    "    # FIGURE OUT WHY. I NEED TO DO THIS (SHOULDN\"T HAVE TO AND IT messes THiNGS UP)\n",
    "    team_scores[team] = team_scores[team][0]\n",
    "team_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ranking Teams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
